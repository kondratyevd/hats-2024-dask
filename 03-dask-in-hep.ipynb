{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask in HEP analyses\n",
    "\n",
    "### Warning: In this talk we will assume some familiarity with the uproot+awkward way of dealing with data.\n",
    "\n",
    "Dask provides an interface for specifying/locating input data and then describing manipulations on that data are organized into a task graph. This task graph can then be executed on local compute or on a cluster. Dask Array and Dask Dataframe deal well with rectangular data. They provide a scalable interface to describe manipulations of data that may not fit into\n",
    "system memory by mapping transformations onto partitions of the data that fit in memory.\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-overview.svg\" width=\"400\" style=\"margin-right: 20px;\"><img src=\"https://docs.dask.org/en/latest/_images/dask-array.svg\" width=\"400\">\n",
    "\n",
    "But in physics we're dealing with [jagged or ragged arrays](https://en.wikipedia.org/wiki/Jagged_array). A ragged array is something like this:\n",
    "\n",
    "```\n",
    "[[1, 2, 3],\n",
    " [4],\n",
    " [],\n",
    " [5, 6]]\n",
    "---------------------\n",
    "type: 4 * var * int64\n",
    "```\n",
    "\n",
    "In the pythonic HEP ecosystem, we deal with those kinds of arrays using [awkward](https://github.com/scikit-hep/awkward). Awkward Arrays are general tree-like data structures, like JSON, but contiguous in memory and operated upon with compiled, vectorized code like NumPy. For more information, please visit the [awkward array docs](https://awkward-array.org/doc/main/index.html) and/or see [previous talks from Jim Pivarski](https://github.com/jpivarski-talks/).\n",
    "\n",
    "### How jagged arrays and histogramming are integrated with dask: awkward array 2.0, dask_awkward, dask_histogram, and coffea\n",
    "\n",
    "<img src=\"img/coffea-upgrade.png\" width=\"600\">\n",
    "\n",
    "Awkward array 2.0 features an improved and streamlined backend with. The backend is using only C and python without any C++ metadata handling. `ak.virtual` delayed computations are replaced by [dask-awkward](https://github.com/dask-contrib/dask-awkward).\n",
    "\n",
    "`dask_awkward` and `dask_histogram` bring delayed, distributed computation to `awkward array 2.0` based analyses and libraries. [Uproot](https://github.com/scikit-hep/uproot5) now provides lazy reading of data via [uproot.dask](https://uproot.readthedocs.io/en/latest/uproot._dask.dask.html).\n",
    "\n",
    "#### Our partitions are split on the event axis since each event is independent and we never run computations that combine more than 1 events.\n",
    "\n",
    "All that provides access to dask at all layers of analysis which yields improved parallelism and better factorization away from compute infrastructure.\n",
    "Coffea, and `NanoEvents` in particular which was almost entirely based on `ak.virtual`, now leverage all this infrastructure.\n",
    "\n",
    "Please see [Lindsey's CHEP 2023 talk](https://indico.jlab.org/event/459/contributions/11533/attachments/9496/13762/CoffeaCHEP_LindseyGray_09052023.pdf) for more info on those upgrades and also see the most recent talks from [Lindsey](https://indico.cern.ch/event/1383972/contributions/5825304/) and [Jim](https://github.com/jpivarski-talks/2023-07-24-tac-hep-tutorial/blob/main/horizontal.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.threaded import get\n",
    "from dask.local import get_sync\n",
    "from dask.optimization import cull, inline, inline_functions, fuse\n",
    "\n",
    "\n",
    "def print_and_return(string):\n",
    "    print(string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def format_str(count, val, nwords):\n",
    "    return f\"word list has {count} occurrences of \" f\"{val}, out of {nwords} words\"\n",
    "\n",
    "\n",
    "dsk = {\n",
    "    \"words\": \"apple orange apple pear orange pear pear\",\n",
    "    \"nwords\": (len, (str.split, \"words\")),\n",
    "    \"val1\": \"orange\",\n",
    "    \"val2\": \"apple\",\n",
    "    \"val3\": \"pear\",\n",
    "    \"count1\": (str.count, \"words\", \"val1\"),\n",
    "    \"count2\": (str.count, \"words\", \"val2\"),\n",
    "    \"count3\": (str.count, \"words\", \"val3\"),\n",
    "    \"format1\": (format_str, \"count1\", \"val1\", \"nwords\"),\n",
    "    \"format2\": (format_str, \"count2\", \"val2\", \"nwords\"),\n",
    "    \"format3\": (format_str, \"count3\", \"val3\", \"nwords\"),\n",
    "    \"print1\": (print_and_return, \"format1\"),\n",
    "    \"print2\": (print_and_return, \"format2\"),\n",
    "    \"print3\": (print_and_return, \"format3\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.base.visualize_dsk(dsk, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\"print1\", \"print2\"]\n",
    "dsk1, dependencies = cull(dsk, outputs)  # remove unnecessary tasks from the graph\n",
    "\n",
    "results = get_sync(dsk1, outputs)\n",
    "\n",
    "dask.base.visualize_dsk(dsk1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk2 = inline(dsk1, dependencies=dependencies)\n",
    "results = get_sync(dsk2, outputs)\n",
    "\n",
    "dask.base.visualize_dsk(dsk2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk3 = inline_functions(dsk2, outputs, [len, str.split], dependencies=dependencies)\n",
    "results = get_sync(dsk3, outputs)\n",
    "\n",
    "dask.base.visualize_dsk(dsk3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk4, dependencies = fuse(dsk3)\n",
    "results = get_sync(dsk4, outputs)\n",
    "\n",
    "dask.base.visualize_dsk(dsk4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(dsk, keys):\n",
    "    dsk1, deps = cull(dsk, keys)\n",
    "    dsk2 = inline(dsk1, dependencies=deps)\n",
    "    dsk3 = inline_functions(dsk2, keys, [len, str.split], dependencies=deps)\n",
    "    dsk4, deps = fuse(dsk3)\n",
    "    return dsk4, deps\n",
    "\n",
    "\n",
    "def optimize_and_get(dsk, keys):\n",
    "    dsk4, deps = fuse(dsk, keys)\n",
    "    return get(dsk4, keys)\n",
    "\n",
    "\n",
    "optimize_and_get(dsk, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.base.visualize_dsk(dsk, verbose=True, color=\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk5, _ = optimize(dsk, outputs)\n",
    "\n",
    "dask.base.visualize_dsk(dsk5, verbose=True, color=\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numba\n",
    "from coffea.dataset_tools import apply_to_fileset\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, BaseSchema\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea import processor\n",
    "\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask as hda\n",
    "\n",
    "# The opendata files are non-standard NanoAOD, so some optional data columns are missing\n",
    "NanoAODSchema.warn_missing_crossrefs = False\n",
    "\n",
    "# The warning emitted below indicates steps_per_file is for initial data exploration\n",
    "# and test. When running at scale there are better ways to specify processing chunks\n",
    "# of files.\n",
    "events, report = NanoEventsFactory.from_root(\n",
    "    {\n",
    "        \"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"\n",
    "    },\n",
    "    metadata={\"dataset\": \"Test\"},\n",
    "    uproot_options={\"allow_read_errors_with_report\": True},\n",
    ").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1\n",
    "\n",
    "Plot the <i>E</i><sub>T</sub><sup>miss</sup> of all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"met\", label=\"$E_{T}^{miss}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(events.MET.pt)\n",
    ")\n",
    "\n",
    "dask.compute(q1_hist, report)[0].plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q1_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2\n",
    "Plot the <i>p</i><sub>T</sub> of all jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"ptj\", label=\"Jet $p_{T}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(ak.flatten(events.Jet.pt))\n",
    ")\n",
    "\n",
    "\n",
    "q2_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q2_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 3\n",
    "Plot the <i>p</i><sub>T</sub> of jets with |<i>η</i>| < 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"ptj\", label=\"Jet $p_{T}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(ak.flatten(events.Jet[abs(events.Jet.eta) < 1].pt))\n",
    ")\n",
    "\n",
    "q3_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q3_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 4\n",
    "Plot the <i>E</i><sub>T</sub><sup>miss</sup> of events that have at least two jets with <i>p</i><sub>T</sub> > 40 GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has2jets = ak.sum(events.Jet.pt > 40, axis=1) >= 2\n",
    "q4_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"met\", label=\"$E_{T}^{miss}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(events[has2jets].MET.pt)\n",
    ")\n",
    "\n",
    "q4_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q4_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 5\n",
    "Plot the <i>E</i><sub>T</sub><sup>miss</sup> of events that have an\n",
    "opposite-charge muon pair with an invariant mass between 60 and 120 GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupair = ak.combinations(events.Muon, 2, fields=[\"mu1\", \"mu2\"])\n",
    "pairmass = (mupair.mu1 + mupair.mu2).mass\n",
    "goodevent = ak.any(\n",
    "    (pairmass > 60) & (pairmass < 120) & (mupair.mu1.charge == -mupair.mu2.charge),\n",
    "    axis=1,\n",
    ")\n",
    "q5_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"met\", label=\"$E_{T}^{miss}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(events[goodevent].MET.pt)\n",
    ")\n",
    "\n",
    "\n",
    "q5_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q5_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 6\n",
    "For events with at least three jets, plot the <i>p</i><sub>T</sub> of the trijet four-momentum that has the invariant mass closest to 172.5 GeV in each event and plot the maximum <i>b</i>-tagging discriminant value among the jets in this trijet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets = ak.zip(\n",
    "    {k: getattr(events.Jet, k) for k in [\"x\", \"y\", \"z\", \"t\", \"btagDeepB\"]},\n",
    "    with_name=\"LorentzVector\",\n",
    "    behavior=events.Jet.behavior,\n",
    ")\n",
    "trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])\n",
    "trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3\n",
    "trijet = ak.flatten(\n",
    "    trijet[ak.singletons(ak.argmin(abs(trijet.p4.mass - 172.5), axis=1))]\n",
    ")\n",
    "maxBtag = np.maximum(\n",
    "    trijet.j1.btagDeepB,\n",
    "    np.maximum(\n",
    "        trijet.j2.btagDeepB,\n",
    "        trijet.j3.btagDeepB,\n",
    "    ),\n",
    ")\n",
    "q6_hists = {\n",
    "    \"trijetpt\": hda.Hist.new.Reg(100, 0, 200, name=\"pt3j\", label=\"Trijet $p_{T}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(trijet.p4.pt),\n",
    "    \"maxbtag\": hda.Hist.new.Reg(\n",
    "        100, 0, 1, name=\"btagDeepB\", label=\"Max jet b-tag score\"\n",
    "    )\n",
    "    .Double()\n",
    "    .fill(maxBtag),\n",
    "}\n",
    "\n",
    "out = dask.compute(q6_hists, report)[0]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "out[\"trijetpt\"].plot1d(ax=ax1, flow=\"none\", yerr=False)\n",
    "out[\"maxbtag\"].plot1d(ax=ax2, flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q6_hists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 7\n",
    "Plot the scalar sum in each event of the <i>p</i><sub>T</sub> of jets with <i>p</i><sub>T</sub> > 30 GeV that are not within 0.4 in Δ<i>R</i> of any light lepton with <i>p</i><sub>T</sub> > 10 GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanjets = events.Jet[\n",
    "    ak.all(events.Jet.metric_table(events.Muon[events.Muon.pt > 10]) >= 0.4, axis=2)\n",
    "    & ak.all(\n",
    "        events.Jet.metric_table(events.Electron[events.Electron.pt > 10]) >= 0.4,\n",
    "        axis=2,\n",
    "    )\n",
    "    & (events.Jet.pt > 30)\n",
    "]\n",
    "q7_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"sumjetpt\", label=\"Jet $\\sum p_{T}$ [GeV]\")\n",
    "    .Double()\n",
    "    .fill(ak.sum(cleanjets.pt, axis=1))\n",
    ")\n",
    "\n",
    "q7_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q7_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 8\n",
    "For events with at least three light leptons and a same-flavor opposite-charge light lepton pair, find such a pair that has the invariant mass closest to 91.2 GeV in each event and plot the transverse mass of the system consisting of the missing tranverse momentum and the highest-<i>p</i><sub>T</sub> light lepton not in this pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"Electron\", \"pdgId\"] = -11 * events.Electron.charge\n",
    "events[\"Muon\", \"pdgId\"] = -13 * events.Muon.charge\n",
    "events[\"leptons\"] = ak.concatenate(\n",
    "    [events.Electron, events.Muon],\n",
    "    axis=1,\n",
    ")\n",
    "events = events[ak.num(events.leptons) >= 3]\n",
    "pair = ak.argcombinations(events.leptons, 2, fields=[\"l1\", \"l2\"])\n",
    "pair = pair[(events.leptons[pair.l1].pdgId == -events.leptons[pair.l2].pdgId)]\n",
    "\n",
    "pair = pair[\n",
    "    ak.singletons(\n",
    "        ak.argmin(\n",
    "            abs((events.leptons[pair.l1] + events.leptons[pair.l2]).mass - 91.2),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "events = events[ak.num(pair) > 0]\n",
    "pair = pair[ak.num(pair) > 0][:, 0]\n",
    "\n",
    "l3 = ak.local_index(events.leptons)\n",
    "l3 = l3[(l3 != pair.l1) & (l3 != pair.l2)]\n",
    "l3 = l3[ak.argmax(events.leptons[l3].pt, axis=1, keepdims=True)]\n",
    "l3 = events.leptons[l3][:, 0]\n",
    "\n",
    "mt = np.sqrt(2 * l3.pt * events.MET.pt * (1 - np.cos(events.MET.delta_phi(l3))))\n",
    "q8_hist = (\n",
    "    hda.Hist.new.Reg(100, 0, 200, name=\"mt\", label=\"$\\ell$-MET transverse mass [GeV]\")\n",
    "    .Double()\n",
    "    .fill(mt)\n",
    ")\n",
    "\n",
    "q8_hist.compute().plot1d(flow=\"none\", yerr=False)\n",
    "dak.necessary_columns(q8_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced fancy analysis\n",
    "Fancy 4-muon analysis, searching for diboson events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    \"ZZto4mu\": {\n",
    "        \"files\": {\n",
    "            \"data/ZZTo4mu.root\": \"Events\",\n",
    "        }\n",
    "    },\n",
    "    \"SMHiggsToZZTo4L\": {\n",
    "        \"files\": {\n",
    "            \"data/SMHiggsToZZTo4L.root\": \"Events\",\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def find_4lep_kernel(events_leptons, builder):\n",
    "    \"\"\"Search for valid 4-lepton combinations from an array of events * leptons {charge, ...}\n",
    "\n",
    "    A valid candidate has two pairs of leptons that each have balanced charge\n",
    "    Outputs an array of events * candidates {indices 0..3} corresponding to all valid\n",
    "    permutations of all valid combinations of unique leptons in each event\n",
    "    (omitting permutations of the pairs)\n",
    "    \"\"\"\n",
    "    for leptons in events_leptons:\n",
    "        builder.begin_list()\n",
    "        nlep = len(leptons)\n",
    "        for i0 in range(nlep):\n",
    "            for i1 in range(i0 + 1, nlep):\n",
    "                if leptons[i0].charge + leptons[i1].charge != 0:\n",
    "                    continue\n",
    "                for i2 in range(nlep):\n",
    "                    for i3 in range(i2 + 1, nlep):\n",
    "                        if len({i0, i1, i2, i3}) < 4:\n",
    "                            continue\n",
    "                        if leptons[i2].charge + leptons[i3].charge != 0:\n",
    "                            continue\n",
    "                        builder.begin_tuple(4)\n",
    "                        builder.index(0).integer(i0)\n",
    "                        builder.index(1).integer(i1)\n",
    "                        builder.index(2).integer(i2)\n",
    "                        builder.index(3).integer(i3)\n",
    "                        builder.end_tuple()\n",
    "        builder.end_list()\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "def find_4lep(events_leptons):\n",
    "    if ak.backend(events_leptons) == \"typetracer\":\n",
    "        # here we fake the output of find_4lep_kernel since\n",
    "        # operating on length-zero data returns the wrong layout!\n",
    "        ak.typetracer.touch_data(\n",
    "            events_leptons.charge\n",
    "        )  # force touching of the necessary data\n",
    "        # Use x = ak.typetracer.length_zero_if_typetracer(events_leptons.charge) or x = ak.typetracer.length_one_if_typetracer(events_leptons.charge)\n",
    "        # if you want a a length-zero/one NumPy-backed array to be returned for your computations\n",
    "        return ak.Array(\n",
    "            ak.Array([[(0, 0, 0, 0)]]).layout.to_typetracer(forget_length=True)\n",
    "        )\n",
    "    return find_4lep_kernel(events_leptons, ak.ArrayBuilder()).snapshot()\n",
    "\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    # The processor can also have an __init__ method\n",
    "\n",
    "    def process(self, events):\n",
    "        dataset_axis = hist.axis.StrCategory(\n",
    "            [], growth=True, name=\"dataset\", label=\"Primary dataset\"\n",
    "        )\n",
    "        mass_axis = hist.axis.Regular(\n",
    "            300, 0, 300, name=\"mass\", label=r\"$m_{\\mu\\mu}$ [GeV]\"\n",
    "        )\n",
    "        pt_axis = hist.axis.Regular(300, 0, 300, name=\"pt\", label=r\"$p_{T,\\mu}$ [GeV]\")\n",
    "\n",
    "        h_nMuons = hda.Hist(\n",
    "            dataset_axis,\n",
    "            hda.hist.hist.axis.IntCategory(\n",
    "                range(6), name=\"nMuons\", label=\"Number of good muons\"\n",
    "            ),\n",
    "            storage=\"weight\",\n",
    "            label=\"Counts\",\n",
    "        )\n",
    "        h_m4mu = hda.hist.Hist(\n",
    "            dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "        h_mZ1 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ2 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu1 = hda.hist.Hist(\n",
    "            dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "        h_ptZ1mu2 = hda.hist.Hist(\n",
    "            dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "\n",
    "        cutflow = dict()\n",
    "\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        muons = ak.zip(\n",
    "            {\n",
    "                \"pt\": events.Muon_pt,\n",
    "                \"eta\": events.Muon_eta,\n",
    "                \"phi\": events.Muon_phi,\n",
    "                \"mass\": events.Muon_mass,\n",
    "                \"charge\": events.Muon_charge,\n",
    "                \"isolation\": events.Muon_pfRelIso03_all,\n",
    "            },\n",
    "            with_name=\"PtEtaPhiMCandidate\",\n",
    "            behavior=candidate.behavior,\n",
    "        )\n",
    "\n",
    "        # make sure they are sorted by transverse momentum\n",
    "        muons = muons[ak.argsort(muons.pt, axis=1)]\n",
    "\n",
    "        cutflow[\"all events\"] = ak.num(muons, axis=0)\n",
    "\n",
    "        # impose some quality and minimum pt cuts on the muons\n",
    "        muons = muons[(muons.pt > 5) & (muons.isolation < 0.2)]\n",
    "        cutflow[\"at least 4 good muons\"] = ak.sum(ak.num(muons) >= 4)\n",
    "        h_nMuons.fill(dataset=dataset, nMuons=ak.num(muons))\n",
    "\n",
    "        # reduce first axis: skip events without enough muons\n",
    "        muons = muons[ak.num(muons) >= 4]\n",
    "\n",
    "        # find all candidates with helper function\n",
    "        fourmuon = dak.map_partitions(find_4lep, muons)\n",
    "        fourmuon = [muons[fourmuon[idx]] for idx in \"0123\"]\n",
    "\n",
    "        fourmuon = ak.zip(\n",
    "            {\n",
    "                \"z1\": ak.zip(\n",
    "                    {\n",
    "                        \"lep1\": fourmuon[0],\n",
    "                        \"lep2\": fourmuon[1],\n",
    "                        \"p4\": fourmuon[0] + fourmuon[1],\n",
    "                    }\n",
    "                ),\n",
    "                \"z2\": ak.zip(\n",
    "                    {\n",
    "                        \"lep1\": fourmuon[2],\n",
    "                        \"lep2\": fourmuon[3],\n",
    "                        \"p4\": fourmuon[2] + fourmuon[3],\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        cutflow[\"at least one candidate\"] = ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # require minimum dimuon mass\n",
    "        fourmuon = fourmuon[(fourmuon.z1.p4.mass > 60.0) & (fourmuon.z2.p4.mass > 20.0)]\n",
    "        cutflow[\"minimum dimuon mass\"] = ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # choose permutation with z1 mass closest to nominal Z boson mass\n",
    "        bestz1 = ak.singletons(ak.argmin(abs(fourmuon.z1.p4.mass - 91.1876), axis=1))\n",
    "        fourmuon = ak.flatten(fourmuon[bestz1])\n",
    "\n",
    "        h_m4mu.fill(\n",
    "            dataset=dataset,\n",
    "            mass=(fourmuon.z1.p4 + fourmuon.z2.p4).mass,\n",
    "        )\n",
    "        h_mZ1.fill(\n",
    "            dataset=dataset,\n",
    "            mass=fourmuon.z1.p4.mass,\n",
    "        )\n",
    "        h_mZ2.fill(\n",
    "            dataset=dataset,\n",
    "            mass=fourmuon.z2.p4.mass,\n",
    "        )\n",
    "        h_ptZ1mu1.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep1.pt,\n",
    "        )\n",
    "        h_ptZ1mu2.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep2.pt,\n",
    "        )\n",
    "        return {\n",
    "            \"nMuons\": h_nMuons,\n",
    "            \"mass\": h_m4mu,\n",
    "            \"mass_z1\": h_mZ1,\n",
    "            \"mass_z2\": h_mZ2,\n",
    "            \"pt_z1_mu1\": h_ptZ1mu1,\n",
    "            \"pt_z1_mu2\": h_ptZ1mu2,\n",
    "            \"cutflow\": {dataset: cutflow},\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compute = apply_to_fileset(\n",
    "    FancyDimuonProcessor(),\n",
    "    fileset,\n",
    "    schemaclass=BaseSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dak.necessary_columns(to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(to_compute, filename=\"unoptimized.pdf\", optimize_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(to_compute, filename=\"optimized.pdf\", optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "(out,) = dask.compute(to_compute)\n",
    "print(out)\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt = (\n",
    "    out[\"ZZto4mu\"][\"cutflow\"][\"ZZto4mu\"][\"all events\"]\n",
    "    + out[\"SMHiggsToZZTo4L\"][\"cutflow\"][\"SMHiggsToZZTo4L\"][\"all events\"]\n",
    ")\n",
    "print(\"Events/s:\", (nevt / elapsed))\n",
    "\n",
    "scaled = {}\n",
    "for (name1, h1), (name2, h2) in zip(\n",
    "    out[\"ZZto4mu\"].items(), out[\"SMHiggsToZZTo4L\"].items()\n",
    "):\n",
    "    if isinstance(h1, hist.Hist) and isinstance(h2, hist.Hist):\n",
    "        scaled[name1] = h1.copy() + h2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scaled[\"nMuons\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass\"][:, :: hist.rebin(4)].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass_z1\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass_z2\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "ax.set_xlim(2, 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"pt_z1_mu1\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"pt_z1_mu2\"].plot1d(ax=ax, overlay=\"dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very hard to convert an analysis from `awkward1` + `coffea` to `awkward2` + `dask` + `coffea`:\n",
    "- Processors are not needed anymore but they are a nice organization tool.\n",
    "- Coffea executors have been removed and dask takes care of that.\n",
    "- Accumulators are also not a thing anymore.\n",
    "- Awkward operations on dask-awkward arrays are automatically dispatched `ak.some_function` -> `dak.some_function`.\n",
    "- Use `import dask_awkward as dak` if you wanna be more explicit and not run operations on eager arrays by accident.\n",
    "- For histogramming, use `import hist.dask as hda`. `hist.dask` histograms behave like empty `hist` histograms, and will return a filled `hist.hist.Hist` when `.compute()` is called.\n",
    "- You'll still need to `import hist` for convenient definition of axes.\n",
    "- If you want to get a complete array or histogram object in memory on your local machine so that you can manipulate it use `array.compute()`, `ahistogram.compute()`, or `dask.compute({\"some\": array, \"another\": histogram})`\n",
    "- This should often be done at the end of code that is constructing arrays, do not prematurely `.compute()` as it can drastically slow down your analysis code.\n",
    "- Use dask clusters and clients to scale your analysis.\n",
    "- Most array operations that are available in `awkward` are available in `dask_awkward`, and if you encounter a problem or missing piece of functionality that you need you should open an issue at the [dask-awkward github page](https://github.com/dask-contrib/dask-awkward).\n",
    "- Functions that cannot be written in pure `awkward` must be wrapped and used in `dak.map_partitions`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
